
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{array,color}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\urldef{\mailsa}\path|{yu-zhao, gaosheng}@bupt.edu.cn|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Pairwise Interaction Embeddings for Knowledge Base Completion}

% a short form should be given in case it is too long for the running head
\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Yu Zhao\and Sheng Gao}
%
\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Beijing University of Posts and Telecommunications, China\\
\mailsa\\
\url{http://www.springer.com/lncs}
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
The abstract should summarize the contents of the paper and should
contain at least 70 and at most 150 words. It should be written using the
\emph{abstract} environment.
\keywords{We would like to encourage you to list your keywords within
the abstract section}
\end{abstract}


\section{Introduction}
Knowledge Bases(KB) such as WordNet\footnote{wordnet.princeton.edu},Freebase\footnote{freebase.com} or Google Knowledge Graph\footnote{google.com/insidesearch/features/search/knowledge.html} are considered as a directed graphs
whose nodes and edges respectively correspond to $entities$ and $relationships$ of the form $(left-entity,relationship,right-entity)$ (denoted$(e_1,r,e_2)$), each of which indicates that there exists a relationship between the two entities. Knowledge bases are extremely useful for human-like reasoning,query expansion \cite{},coreference resolution \cite{}, question answering(Siri,Speaktoit,iris,SimSimi) and many other AI areas like in Natural Language Processing(NLP) for word-sense disambiguation, in computer vision(CV) for scene classification,etc. However, they always suffer from incompleteness and a lack of reasoning capability.

There is a vast amount of work on extending knowledge bases using patterns or classifiers applied to large text corpora. However, not all common knowledge is expressed in the text explicitly while it is obvious to human being. Our work focuses on modeling the knowledge bases, with the goal of providing an efficient tool to predict the likely truth of additional facts based on the existing facts, without requiring extra knowledge. Such factual, common sense reasoning ia available and useful to people. For example, when we know the facts that ``Bill Clinton was born in Arkansas'' and ``Arkansas belongs to United States'', this would allow us to answer the question like ``what's the Bill Clintion's nationality?''.

In this paper, we propose a new model, called Pairwise Interaction Embedding model$\bf{(PIE)}$, to learn the semantics of the entities and relationships in knowledge bases and to compute how likely the additional fact is true. We consider three assumptions that: 1. the two elements in the pairs (such as $(e_1,r)$,$(e_2,r)$ and $(e_1,e_2)$) generated by the triplets $(e_1,r,e_2)$ would interacted with each other directly(the scoring function consist of three parts); 2. there exists  semantic differences when the entity $e_1$ interacted with other entity $e_2$ (such as $(e_1,e_2)$) and interacted with the relationship (such as $(e_1,r)$)(the representation of the entity consist of two components which the first one is responsible for the interaction with other entity and the second one is responsible for the relationship);3.there exists syntax differences when the relationship interacted left-entity and the right-entity(the representation of the relationship consist of two components which the first one is for left-entity and the second one is for right-entity). The scoring function of the $\bf{PIE}$ model which is trained to assign high score to plausible triplets of the knowledge base, relies on a compact distributed representation that all entities and relationships are represented as two vectors: each vector is in the same low (e.g. 50) dimensional embedding vector space. Our experiment in Section $\ref{experiment}$ demonstrate that the PIE model can significantly outperform state-of-the-art methods in link prediction on real-word KBs.

In the reminder of the paper, we discuss some related methods in Section $\ref{related-work}$ and describe our model in Section $\ref{piem}$. Then we describe experiments on several real world Knowledge Bases, and provide
comparisons with state-of-the-art methods in Section $\ref{experiment}$. We finally conclude by sketching some future work directions in Section $\ref{conclusion}$.


\section{Related Work}
\label{related-work}
There is a lot of related models on embedding knowledge bases. These models map the entities to a vector,
and map the relationship to a vector, matrix or tensor. Each model computes a score on a triplet using a function $g(e_1,r,e_2)$ measuring how likely the triplet is correct. We detail some related model in the following.

$\bf{Structured\quad Embedding}(SE)$  The main idea behind the model, Bordes et al.\cite{}, is transforming
 the entity embedding vector $e_1$ and $e_2$ by the corresponding left and right hand relation matrices for the
 relationship $r$ and then measuring the similarity of the triplet according to the 1-norm distance in the transform
embedding space.The similarity function for a given entity is thus defined as:
\begin{equation}
g(e_1 ,r,e_2 ) = ||W_{rA} e_1  - W_{rB} e_2 ||_1  \ ,
\end{equation}

where the model assign for the given relationship $r$ a pair embedding $W_r= \left(W_{rA}, W_{rB}\right)$,
$W_{rA}, W_{rB} \in \mathbb{R}^{k \times k}$ are both parameter matrices of relationship $r$. Entities are modeled in a k-dimensional vector space, $e_1,e_2 \in \mathbb{R}^k$. This similarity-based model scores correct triplet
lower. If the entities most certainly in a relation, it's score may equal 0. All other functions are trained to score
the correct triplets higher. This model independently map the pair of entities to a common embedding space, it cause a
problem that the parameters of the two entity embedding vectors do not interact with each other. Like SE model mapping a relationship to a pair of parameters, respectively interact with left entity and the right entity,
our PIE Model also maps the relationship to a pair of parameters, it is the same idea to some extent.

$\bf{Semantic\quad Matching\quad Energy\quad Model(SME)}$ The main motivation of the model, Bordes et al.\cite{}, is that entities and relationships would share the same kind of representation. It maps all symbols into the same space amount to deleting the usual conceptual difference between entities and relationships. The energy-based model is encoded using a neural network,whose parallel architecture is based on the intuition that the relationship should first interacted with the left and right entity separately to extract the relevant components, and then compute these semantic combinations. The scoring function is as follows:
\begin{equation}
g(e_1 ,r,e_2 ) = (W_1 e_1  \otimes W_{rel,1} e_r  + b_1 )^T (W_2 e_2  \otimes W_{rel,2} e_r  + b_2 ) \ ,
\end{equation}

where $\otimes$ is Hadamard products, $W_1,W_{rel,1},W_2,W_{rel,2} \in \mathbb{R}^{d \times d}$ and $b_1,b_2 \in \mathbb{R}^{d \times 1}$ are parameters
that are shared by all relations.

$\bf{Neural\quad Tensor\quad Networks(NTN)}$ The model was introduced by Socher et al.\cite{} and tackless the issue of weak entity
vector interaction through replaces a standard linear neural network layer with a bilinear tensor layer that directly relates the two entity vectors across multiple dimensions. The main intuition of the model is that each relationship would have different parts of semantic word vector space. Each slice of the tensor is responsible for one type of entity pair or instantiation of a relation. The NTN-based scoring function:
\begin{equation}
g(e_1 ,r,e_2 ) = u_r ^T f(e_1 ^T W_r ^{[1:\kappa]} e_2  + V_r \left[ {_{e_2 }^{e_1 } } \right] + b_r ) \ ,
\end{equation}

where $V_r \in \mathbb{R}^{\kappa \times 2d}$ and $u_r \in \mathbb{R}^\kappa, b_r \in \mathbb{R}^\kappa$, $f=tanh$ is a standard nonlinearity applied element-wise, $W_r^{[1:k]} \in \mathbb{R}^{d \times d \times k}$ is a tensor and the bilinear tensor product $e_1^TW_r^{[1:\kappa]}e_2$ results in a vector $h \in \mathbb{R}^\kappa$, where each entity is computed by one slice $i = 1,...\kappa$ of the tensor: $h_i =e_1^TW_r^{[i]}e_2$.

$\bf{Translating\quad Embedding(TransE)}$ Bordes et al.\cite{} propose the method which models relationships by interpreting them as translations operating on the low-dimensional embedding of the entities. The similarity scoring function is as follow:
\begin{equation}
g(e_1,r_,e_2) = ||e_1+e_r-e_2||_p \ .
\end{equation}

\section{Pairwise Interaction Embedding Model}
\label{piem}

This section introduces the pairwise interaction embedding model that predicts
the relation by learning vector representations for them. For the formalization,
we use the notation: E is the set of all entities, R the set of all relationships.
The existing knowledge base is given by $KB \in E \times R \times E$.

%As shown
%in Figure ~\ref{pie-fig},we consider the knowledge base is a tensor (3-ways array)
%consist of $\it{E}$,$\it{R}$ and $\it{E}$.We use CP algorithm to decompose the tensor,
% but adding some constraints.

%\begin{figure}[h]
%\begin{center}
%\framebox[4.0in]{$\;$}
%\includegraphics[width=0.9\textwidth]{pie_new.eps}
%\end{center}
%\caption{Pairwise Interaction Embedding Model.}
%\label{pie-fig}
%\end{figure}

\subsection{PIE Model for Link Prediction}

Given a training set $S$ of triplets $(e_1 ,r,e_2 )$ composed of two entities
$e_1$,$e_2 \in E$ and a relationship $r \in R$, which states each pair of
two entities $(e_1, e_2)$ are in a certain relationship $\it{r}$,our goal is to
predict the possible truth of additional facts based purely on the other existing
relations in the knowledge base. Sometimes we also consider this task is link prediction
in an existing network of relationships between entity nodes.
Like many previous models, our model also learns vector embeddings of the entities
and the relationships, but there is some important different aspect. Entities can be modeled
in a low dimensional vector space and each entity and relationship is represented by two embedding vectors,
 $e_1  = (e_{1A} ,e_{1B} )$,$e_2  = (e_{2A} ,e_{2B} )$ and $r  = (e_{rA} ,e_{rB} )$.
 The basic idea behind our model is that it exists some difference from the interaction
of two entities to the interaction of entity and relationship. We use the first part of
entity vectors $e_{1A}^Te_{2A}$ to fix the issue of the two entities vector interaction,
 and use the second part of the left entity $e_{1B}$ times the the first part of the relationship
 embedding $e_{rA}$ ,$e_{1B}^Te_{rA}$, to fix the issue of the left entity and relationship vector interaction,
 and use the second part of the right entity $e_{2B}$ times the the second part of the relationship
 embedding $e_{rB}$, $e_{2B}^Te_{rB}$ to fix the issue of the right entity and relationship vector interaction.
The model compute a score of how likely it is that two entities are in a certain relationship by the following
PIE function:
\begin{equation}
g(e_1 ,r,e_2 ) = e_{1A} ^T e_{2A}  + e_{1B} ^T e_{rA}  + e_{2B} ^T e_{rB} \ ,
\end{equation}

where $e_{1A}, e_{1B}, e_{2A}  ,e_{2B} , e_{rA}$ and $e_{rB} \in \mathbb{R} ^k$.

\subsection{Training Objective and Learning Algorithm}

Our model is trained with contrastive max-margin objective functions. The main idea
is that each triplet in the $S^{(i)}  = (e_1 ^{(i)} ,r^{(i)} ,e_2 ^{(i)} )$ should
receive a higher score than a triplet in which one of the entities is replaced with a
random entity.
To learn such embeddings, we minimize the margin-based ranking criterion over the training set:
\begin{equation}
J(\Omega ) = \sum\limits_{(e_1 ,r,e_2 ) \in S} {\sum\limits_{(e_1 ',r,e_2 ') \in S'} {\max \{ 0,\gamma  - g(e_1 ,r,e_2 ) + g(e_1 ',r,e_2 ')} } \} \ ,
\end{equation}

where $\gamma$, a positive real number, is a margin hyperparameter, and
\begin{equation}
S_{(e_1 ,r,e_2 )} ' = \{ (e_1 ',r,e_2 )|e_1 ' \in E\}  \cup \{ (e_1 ,r,e_2 ')|e_2 ' \in E\} \ .
\end{equation}

The corrupted relations consist of training relations with either the left entity or
right entity replaced by a random entity, which can appear at that position in that relation.
We don't replace both the left and right entity at the same time.

To train the entities parameters $E = (E_A,E_B)$ and the relationships parameter $R=(E_{rA},E_{rB})$ of
our model we use stochastic gradient descent(SGD) to optimize with the additional constraints
that the $L_2$-norm of the embeddings of both the entities and relationships equal one.
Like other previous embedding-based model[???], the constraint is very essential for PIE model.
If $||e_{1A}^{(i)} ||_2 = 1$ and $||e_{2A}^{(i)} ||_2 = 1$, ${e^{(i)}}_{1A}^Te_{2A}^{(i)}$ is equal with the cosine distance between the embedding $e_{1A}^{(i)}$ and $e_{2A}^{(i)}$, $Cos(e^{(i)} _{1A} ,e^{(i)} _{2A} ) = {\raise0.7ex\hbox{${{e^{(i)}} _{1A} ^T e^{(i)} _{2A} }$} \!\mathord{\left/
 {\vphantom {{{e^{(i)}} _{1A} ^T e^{(i)} _{2A} } {||e^{(i)} _{1A} ||_2 \times ||e^{(i)} _{2A} ||_2}}}\right.\kern-\nulldelimiterspace}
\!\lower0.7ex\hbox{${||e^{(i)} _{1A} ||_2 \times ||e^{(i)} _{2A} ||_2}$}} ={e^{(i)}} _{1A} ^T e^{(i)} _{2A}$.

The detailed learning algorithm is described in Algorithm 1. We initialize all the embedding for entities $(E_A,E_B)$
 and relationships $(E_rA,E_rB)$ with Gaussian distribution. Then we iterate the following procedure:a small set of triplets
 is sampled from the training set, and will serve as the training triplets of the minibatch. For each positive triplet, we select a random entity to replace its left or right position, sampling a single corrupted(negative) triplet.
 The parameter are then updated by taking a gradient step. The gradient step requires a learning rate of $\lambda$.
 Finally both the embedding vectors of entities and relationships are normalized.The algorithm procedure is iterated for
  a given number of iterations.


\hrulefill{\hrule height 0.8pt}
$\bf{Algorithm\ 1}$ Learning PIE \\
\hrulefill{\hrule height 0.5pt}
$\bf{input}$ Training Set $S = \left\{ {\left( {e_1 ,r,e_2 } \right)} \right\}$ ,entities sets \textit{E}, relation sets \textit{R}, margin $\gamma$ , embeddings dimension $\kappa$ .
    \begin{enumerate}
        \item $\bf{initialize}$ $r_A,r_B \gets Gaussian(0,1)/10$ for each r $\in$ \textit{R}
        \item $\qquad\quad\quad\ \ e_A,e_B \gets Gaussian(0,1)/10$ for each e $\in$ \textit{E}
        \item $\bf{loop}$
        \item $\quad S_{batch}\in sample(S,b)$  \//\//sample a minibatch of size b
        \item $\quad T_{batch}\in \phi$  \//\//initialize the set of pairs of triplets
        \item $\quad \bf{for}$ $\left( {e_1 ,r,e_2 } \right)\in S_{batch}$ $\bf{do}$
        \item $\quad\quad\ $    $\left( {e_1' ,r ,e_2'} \right) \gets $ $sample S_{(e_{1,r,e_2 } )}'$ \//\// sample a corrupted triplet
        \item $\quad\quad\ \ T_{batch} \gets T_{batch} \cup {((e_1,r,e_2),(e_1',r,e_2'))}$
        \item $\quad \bf{end\ for}$
        \item $\ \ $ Update embedding w.r.t \\
          \rightline{$\sum\limits_{((e_1 ,r,e_2 ),(e_1' ,r,e_2' )) \in T_{batch} } {\nabla [\gamma  - g(e_1 ,r,e_2 ) + g(e_1' ,r,e_2' )]}$}
        \item $\quad r_A \gets r_A/\| r_A\|,\ r_B \gets r_B/\| r_B\|$ for each relationship r $\in$ \textit{R}
        \item $\quad e_A \gets e_A/\| e_A\|,\ e_B \gets e_B/\| e_B\|$ for each entity e $\in$ \textit{E}
        \item $\bf{end loop}$
    \end{enumerate}
\hrulefill {\hrule height 0.8pt}

\section{Experiments}
\label{experiment}

Our proposed model, PIE, is evaluated on the data set extracted from the Knowledge Bases: $Freebase$. We compare it with several state-of-the-art models for link prediction.

\subsection{Data Sets}

$\bf{Freebase}$ Freebase is a huge and growing KB of general facts; there are currently around 1.2 billion triplets and more than 80 million entities. A small data set has 592,213 triplets with 14,951 entities and 1,345 relationships which were randomly split. This data set is denoted FB15k\cite{} in the rest of this section.

Table \ref{data-stcs} gives the statistics of the databases.

\begin{table}[t]
\caption{The staticstics for FB15k}
\label{data-stcs}
\begin{center}
\begin{tabular}{c|c|c|c|c|c}

\hline
      & Entities & Relationships & Train & Valid & Test \\
\hline
FB15k & 14951 & 1345 & 483142 & 50000 & 59071 \\
\hline	

\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{\#parameters of several models. $n_e$ and $n_r$ are the number of entities and relationships; $\kappa$ the embedding dimension.}
\label{para-stcs}
\begin{center}
\begin{tabular}{c|m{3.5cm} }

\hline
  Model   &  \#parameters \\
\hline
SE & $O(n_e\kappa + 2n_r\kappa^2)$ \\
SME$(Linear)$ & $O(n_e\kappa + n_r\kappa+4\kappa^2)$ \\
SME$(Binear)$ & $O(n_e\kappa + n_r\kappa+2\kappa^3)$ \\
TransE & $O(n_e\kappa + n_r\kappa)$ \\
\hline
PIE & $O(2n_e\kappa + 2n_r\kappa)$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Experiment setup}
In particular, we are interested in how the PIE model do link prediction in knowledge base.
We use the ranking procedure\cite{?} for evaluation. For each test triplet,
we removed the head and replaced it by each of the entities of the dictionary in turn.
The function value $g(e_1',r,e_2)$ of the negative triplets are first computed by our model
and then sorted by descending order. Finally we can get the exact rank of the correct entity.
We repeated the whole procedure while removing the tail instead of the head of the test triplet.
The mean of those predicted ranks and the $hits@10$, i.e.the proportion of correct entities ranked in
the top 10 are reported in result table. Using the same approach, we also calculate the mean rank and $hits@10$ of the relationship prediction of the test triplets.

For experiment with PIE model, we selected the learning rate $\lambda$ for the stochastic gradient
descent among \{0.001,0.01\}, the margin $\gamma$ among \{1,10\} and the embedding dimension $\kappa$ in
a range of \{50,100\} on the validation set of each data set. Finally our model configuration on both data sets are
\{$\kappa = 50, \lambda = 0.001, \gamma=1$\}.For all data sets, training time are limited to at most 500 epoches over the
training set.

\subsection{Results of link prediction}
In our experiments, we consider two type of link prediction task:
\begin{enumerate}
    \item The first one is to predict the head or tail of the triplet in the testing data set.
    \item The second is to predict the relationship between the test entity pair.
\end{enumerate}

Table \ref{e-p-r} shows the results on the two data sets with other compared methods.
From the results our proposed PIE outperforms all counterparts on all metrics.

\begin{table}[t]
\caption{Link prediction results of FB15k: 1.To predict the left entity or right entity of the triplet;2.To predict the relationship  of the triplet.Test performance of the different models}
\label{e-p-r}
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
 DataSet:FB15k   & Evaluation Method & SE & SME$\left( Linear \right)$ & SME$\left( Binear \right)$  & TransE & PIE\\
\hline
    \multirow{2}{*}{Entity Prediction}
    & Mean\_Rank                & 273 & 274  & 284   & 243    & $\bf{192.35}$ \\
    \cline{2-7}
    & Hits@10 $\left(\%\right)$ & 28.8  & 30.7 & 31.3  & 34.9   & $\bf{ 42.32}$ \\
\hline
    \multirow{2}{*}{Relationship Prediction}
    & Mean\_Rank                & 611.60 &      &       & 101.91 & $\bf{3.32 } $ \\
    \cline{2-7}
    & Hits@10 $\left(\%\right)$ & 22.73  &      &       & 73.08  & $\bf{98.10} $\\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Case study}

\begin{table}[t]
\caption{Example prediction on the FB15k test set using PIE. $\bf{Bold}$ indicates the test
triplet's true right-entity and $\it{italics}$ other true right-entities present in the training set.}
\label{case-study}
\begin{center}
\begin{tabular}{|c|c|}
\hline
Input $\left(Left-Entity\quad and \quad Relationship\right)$ & Predict Right-entity \\
\hline
&  \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Example prediction on the FB15k test set using PIE. $\bf{Bold}$ indicates the test
triplet's true left-entity and $\it{italics}$ other true left-entities present in the training set.}
\label{case-study}
\begin{center}
\begin{tabular}{|c|c|}
\hline
Input $\left(Left-Entity\quad and \quad Relationship\right)$ & Predict Right-entity \\
\hline
/m/07b_l,	/m/0vzm,	/m/0f2rq,	/m/05mph,	/m/01n7q,	/m/03l2n,	/m/03v0t,	/m/013m43,	/m/0f2s6,	/m/04ly1 & /location/location/contains	/m/013m43 \\
\hline
\end{tabular}
\end{center}
\end{table}



\begin{table}[t]
\caption{Example prediction on the FB15k test set using PIE. $\bf{Bold}$ indicates the test
triplet's true relationship and $\it{italics}$ other true relationships present in the training set.}
\label{case-study2}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Input $\left(Left-Entity\right)$ & Predict Relationship & Input $\left(Right-Entity\right)$\\
\hline
&  &\\
\hline
\end{tabular}
\end{center}
\end{table}

\section{Conclusion }
\label{conclusion}

\subsubsection*{Acknowledgments.} The heading should be treated as a
subsubsection heading and should not be assigned a number.

\section{The References Section}\label{references}

In order to permit cross referencing within LNCS-Online, and eventually
between different publishers and their online databases, LNCS will,
from now on, be standardizing the format of the references. This new
feature will increase the visibility of publications and facilitate
academic research considerably. Please base your references on the
examples below. References that don't adhere to this style will be
reformatted by Springer. You should therefore check your references
thoroughly when you receive the final pdf of your paper.
The reference section must be complete. You may not omit references.
Instructions as to where to find a fuller version of the references are
not permissible.

We only accept references written using the latin alphabet. If the title
of the book you are referring to is in Russian or Chinese, then please write
(in Russian) or (in Chinese) at the end of the transcript or translation
of the title.

The following section shows a sample reference list with entries for
journal articles \cite{jour}, an LNCS chapter \cite{lncschap}, a book
\cite{book}, proceedings without editors \cite{proceeding1} and
\cite{proceeding2}, as well as a URL \cite{url}.
Please note that proceedings published in LNCS are not cited with their
full titles, but with their acronyms!

\begin{thebibliography}{4}

\bibitem{jour} Smith, T.F., Waterman, M.S.: Identification of Common Molecular
Subsequences. J. Mol. Biol. 147, 195--197 (1981)

\bibitem{lncschap} May, P., Ehrlich, H.C., Steinke, T.: ZIB Structure Prediction Pipeline:
Composing a Complex Biological Workflow through Web Services. In: Nagel,
W.E., Walter, W.V., Lehner, W. (eds.) Euro-Par 2006. LNCS, vol. 4128,
pp. 1148--1158. Springer, Heidelberg (2006)

\bibitem{book} Foster, I., Kesselman, C.: The Grid: Blueprint for a New Computing
Infrastructure. Morgan Kaufmann, San Francisco (1999)

\bibitem{proceeding1} Czajkowski, K., Fitzgerald, S., Foster, I., Kesselman, C.: Grid
Information Services for Distributed Resource Sharing. In: 10th IEEE
International Symposium on High Performance Distributed Computing, pp.
181--184. IEEE Press, New York (2001)

\bibitem{proceeding2} Foster, I., Kesselman, C., Nick, J., Tuecke, S.: The Physiology of the
Grid: an Open Grid Services Architecture for Distributed Systems
Integration. Technical report, Global Grid Forum (2002)

\bibitem{url} National Center for Biotechnology Information, \url{http://www.ncbi.nlm.nih.gov}

\end{thebibliography}


\section*{Appendix: Springer-Author Discount}

LNCS authors are entitled to a 33.3\% discount off all Springer
publications. Before placing an order, the author should send an email,
giving full details of his or her Springer publication,
to \url{orders-HD-individuals@springer.com} to obtain a so-called token. This token is a
number, which must be entered when placing an order via the Internet, in
order to obtain the discount.

\section{Checklist of Items to be Sent to Volume Editors}
Here is a checklist of everything the volume editor requires from you:


\begin{itemize}
\settowidth{\leftmargin}{{\Large$\square$}}\advance\leftmargin\labelsep
\itemsep8pt\relax
\renewcommand\labelitemi{{\lower1.5pt\hbox{\Large$\square$}}}

\item The final \LaTeX{} source files
\item A final PDF file
\item A copyright form, signed by one author on behalf of all of the
authors of the paper.
\item A readme giving the name and email address of the
corresponding author.
\end{itemize}
\end{document}
